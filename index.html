<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üë®‚Äçüíª</text></svg>">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Kunchang Li, Li Kunchang, Kunchang, SIAT, UCAS, MMLab, Computer Vision, Video Understanding, Artificial Intelligence"> 
<meta name="description" content="Kunchang Li is an Ph.D. Student at UCAS.">
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Kunchang Li</title>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');
</script>
</head>

<body>

<nav class="navbar navbar-dark navbar-expand-lg fixed-top">
    <div id="layout-menu">
        <a href="#research">Research</a>
        <a href="#internships">Internships</a>
        <a href="#honors">Honors</a>
        <a href="#services">Services</a>
    </div>
</nav>

<div id="layout-content" style="margin-top:25px">
<table>
    <tbody>
        <tr>
            <td width="670">
                <div id="toptitle">
                <h1>Kunchang Li</h1>
				</div>
				<p>
					<h3>Ph.D. student</h3>
					<a href="https://mmlab.siat.ac.cn/">MMLab@SIAT</a>, UCAS</br>
					Email: kc.li[at]siat.ac.cn</br>
				</p>
				<p>
					<a href="https://github.com/Andy1621"><img src="pics/github.png" height="30px"></a>
					<a href="https://scholar.google.com/citations?user=D4tLSbsAAAAJ"><img src="pics/google_scholar.png" height="30px"></a>
					<a href="https://twitter.com/likunchang1998"><img src="pics/x.png" height="30px"></a>
					<a href="https://www.linkedin.com/in/kunchangli/"><img src="pics/linkedin.png" height="30px"></a>
					<a href="https://www.zhihu.com/people/li-kun-chang-98"><img src="pics/zhihu.png" height="30px"></a>
					<a href="https://www.youtube.com/channel/UC6xgJnBeCXK5gghwRLv-9bA"><img src="pics/youtube.png" height="30px"></a>
				</p>
				<!-- üì¢ <font color="red">We're seeking driven postgraduates for our Video Foundation Models team at Shanghai AI Lab. Interested? Reach out! üòÑ</font> -->
				üôè <font color="red">I'm actively pursuing career opportunities in multimodal video understanding and generation. Feel free to reach out for potential collaborations.</font>
			</td>
			<td><img src="pics/likunchang.jpeg" height="250"></br></td>
		<tr>
	</tbody>
</table>

<h2>Biography <a href="https://github.com/Andy1621/Andy1621/blob/main/CV_LIKUNCHANG.pdf">[CV]</a></h2>
<p>
	<div style="text-align:justify">I am a 4th-year Ph.D. student at <a href="http://english.siat.cas.cn/">Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences (SIAT)</a>. My advisors are <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ">Yu Qiao</a> and <a href="https://scholar.google.com/citations?user=hD948dkAAAAJ">Yali Wang</a>. I received the B.S. degree from the Software School, Beihang University in 2020. Currently, I am a Reseach Intern at <a href="https://github.com/OpenGVLab">OpenGVLab, Shanghai AI Laboratory</a>. I was fortunte to be involved in internship program at SenseTime and Megvii.</div>
</p>
<p>
	<div style="text-align:justify">I am interested in video understanding, efficient architecture, multi-modality learning and generative AI. Most of my research is about <b>video foundation model</b>, including model design, large-scale pretraining, dataset collection and benchamrk evaluation.</div>
</p>

<div id="research">
<h2>Research</h2>
* refers to the co-first authors. Representative papers are <span class="highlight">highlighted</span>. <br>
<b>All my works are open-sourced on <a href="https://github.com/Andy1621">GitHub</a></b>, and the full paper list can be found on <a href="https://scholar.google.com/citations?user=D4tLSbsAAAAJ">Google Scholar</a>. 

<ul>
	<li>
		<div class="highlight">
		<a href="https://arxiv.org/abs/2403.15377">InternVideo2: Scaling Foundation Models for Multimodal Video Understanding</a>
		<br>Yi Wang*, <b>Kunchang Li*</b>, Xinhao Li*, Jiashuo Yu*, Yinan He*, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang.<br>
		<em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2024.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2403.15377">Paper</a>]
			[<a href="https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2">Code</a>]
		</p>
		</div>
	</li>
	<li>
		<div>
		<a href="https://arxiv.org/abs/2403.06977">VideoMamba: State Space Model for Efficient Video Understanding</a>
		<br><b>Kunchang Li</b>, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, Yu Qiao.<br>
		<em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2024.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2403.06977">Paper</a>]
			[<a href="https://github.com/OpenGVLab/VideoMamba">Code</a>]
			[<a href="https://huggingface.co/spaces/OpenGVLab/VideoMamba">Demo</a>]
			[<a href="https://zhuanlan.zhihu.com/p/688140932">Blog</a>]
		</p>
		</div>
	</li>
	<li>
		<div class="highlight">
		<a href="https://arxiv.org/abs/2311.17005">MVBench: A Comprehensive Multi-modal Video Understanding Benchmark</a>
		<br><b>Kunchang Li</b>, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, Yu Qiao.<br>
		<em>Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2024. &nbsp <font color="red"><strong>(Highlight, Top 3%)</strong></font></br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2311.17005">Paper</a>]
			[<a href="https://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat2">Code</a>]
			[<a href="https://huggingface.co/spaces/OpenGVLab/VideoChat2">Demo</a>]
			[<a href="https://huggingface.co/datasets/OpenGVLab/VideoChat2-IT">Dataset</a>]
			[<a href="https://huggingface.co/datasets/OpenGVLab/MVBench">Benchamrk</a>]
			[<a href="https://huggingface.co/spaces/OpenGVLab/MVBench_Leaderboard">Leaderboard</a>]
			[<a href="https://www.youtube.com/watch?v=OMXlbt7A2OU">Video</a>]
			[<a href="https://zhuanlan.zhihu.com/p/669658267">Blog</a>]
		</p>
		</div>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2401.09414">Vlogger: Make Your Dream A Vlog</a>
		<br>Shaobin Zhuang, <b>Kunchang Li</b>, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, Yali Wang.<br>
		<em>Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2024.</br>
		<p style="margin-top:3px">
			[<a href="https://zhuangshaobin.github.io/Vlogger.github.io/">Project</a>]
			[<a href="https://arxiv.org/abs/2401.09414">Paper</a>]
			[<a href="https://github.com/zhuangshaobin/Vlogger">Code</a>]
			[<a href="https://huggingface.co/spaces/GrayShine/Vlogger-ShowMaker">Demo</a>]
			[<a href="https://www.youtube.com/watch?v=ZRD1-jHbEGk">Video</a>]
			[<a href="https://zhuanlan.zhihu.com/p/678510731">Blog</a>]
		</p>
	</li>
	<li>
		<div>
		<a href="https://arxiv.org/abs/2307.06942">InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation.</a>
		<br>Yi Wang*, Yinan He*, Yizhuo Li*, <b>Kunchang Li</b>, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Conghui He, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, Yu Qiao.<br>
		<em>International Conference on Learning Representations</em> (<b>ICLR</b>), 2024.  &nbsp <font color="red"><strong>(Spotlght, Top 6%)</strong></font></br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2307.06942">Paper</a>]
			[<a href="https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid">Code</a>]
			[<a href="https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo1">Dataset</a>]
		</p>
		</div>
	</li>
	<li>
		<div class="highlight">
		<a href="https://arxiv.org/abs/2201.09450">UniFormer: Unifying Convolution and Self-attention for Visual Recognition</a>
		<br><b>Kunchang Li*</b>, Yali Wang*, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, Yu Qiao.<br>
		<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (<b>TPAMI</b>), 2023.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2201.09450">Paper</a>]
			[<a href="https://github.com/Sense-X/UniFormer">Code</a>]
			[<a href="https://huggingface.co/spaces/Andy1621/uniformer_light">Demo</a>]
			[<a href="https://zhuanlan.zhihu.com/p/461361343">Blog</a>]
		</p>
		</div>
	</li>
	<li>
		<div>
		<a href="https://arxiv.org/abs/2305.06355">VideoChat: Chat-Centric Video Understanding</a>
		<br><b>Kunchang Li*</b>, Yinan He*, Yi Wang*, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, Yu Qiao.<br>
		<em>Arxiv</em>, 2023.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2305.06355">Paper</a>]
			[<a href="https://github.com/OpenGVLab/Ask-Anything">Code</a>]
			[<a href="https://vchat.opengvlab.com/">Demo</a>]
			[<a href="https://zhuanlan.zhihu.com/p/628712512">Blog</a>]
		</p>
		</div>
	</li>
	<li>
		<div class="highlight">
		<a href="https://arxiv.org/abs/2303.16058">Unmasked Teacher: Towards Training-Efficient Video Foundation Models</a>
		<br><b>Kunchang Li</b>, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, Yu Qiao.<br>
		<em>International Conference on Computer Vision</em> (<b>ICCV</b>), 2023. &nbsp <font color="red"><strong>(Oral, Top 2%)</strong></font></br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2303.16058">Paper</a>]
			[<a href="https://github.com/OpenGVLab/unmasked_teacher">Code</a>]
			[<a href="https://zhuanlan.zhihu.com/p/618221217">Blog</a>]
		</p>
		</div>
	</li>
	<li>
		<div class="highlight">
		<a href="https://arxiv.org/abs/2211.09552">UniFormerV2: Unlocking the Potential of Image ViTs for Video Understanding</a>
		<br><b>Kunchang Li</b>, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Limin Wang, Yu Qiao.<br>
		<em>International Conference on Computer Vision</em> (<b>ICCV</b>), 2023.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2211.09552">Paper</a>]
			[<a href="https://github.com/OpenGVLab/UniFormerV2">Code</a>]
			[<a href="https://zhuanlan.zhihu.com/p/584669411">Blog</a>]
		</p>
		</div>
	</li>
	<li>
		<div class="highlight">
		<a href="https://arxiv.org/abs/2212.03191">InternVideo: General Video Foundation Models via Generative and Discriminative Learning.</a>
		<br>Yi Wang*, <b>Kunchang Li*</b>, Yizhuo Li*, Yinan He*, Bingkun Huang*, Zhiyu Zhao*, Hongjie Zhang*, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, Yu Qiao.<br>
		<em>Arxiv</em>, 2022.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2212.03191">Paper</a>]
			[<a href="https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo1">Code</a>]
		</p>
		</div>
	</li>
	<li>
		<div>
		<a href="https://arxiv.org/abs/2205.14871">You Only Need 90K Parameters to Adapt Light</a>
		<br>Ziteng Cui, <b>Kunchang Li</b>, Lin Gu, Shenghan Su, Peng Gao, Zhengkai Jiang, Yu Qiao, Tatsuya Harada.<br>
		<em>The British Machine Vision Conference</em> (<b>BMVC</b>), 2022.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2205.14871">Paper</a>]
			[<a href="https://github.com/cuiziteng/Illumination-Adaptive-Transformer">Code</a>]
			[<a href="https://zhuanlan.zhihu.com/p/535695807">Blog</a>]
		</p>
		</div>
	</li>
	<li>
		<div>
		<a href="https://arxiv.org/abs/2112.02413">PointCLIP: Point Cloud Understanding by CLIP</a>
		<br>Renrui Zhang, Ziyu Guo, Wei Zhang, <b>Kunchang Li</b>, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, Hongsheng Li.<br>
		<em>Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2022.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2112.02413">Paper</a>]
			[<a href="https://github.com/ZrrSkywalker/PointCLIP">Code</a>]
		</p>
		</div>
	</li>
	<li>
		<div>
		<a href="https://arxiv.org/abs/2111.12527">MorphMLP: An Efficient MLP-Like Backbone for Spatial-Temporal Representation Learning</a>
		<br>David Junhao Zhang*, <b>Kunchang Li*</b>, Yali Wang, Yunpeng Chen, Shashwat Chandra, Yu Qiao, Luoqi Liu, Mike Zheng Shou.<br>
		<em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2022.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2111.12527">Paper</a>]
			[<a href="https://github.com/MTLab/MorphMLP">Code</a>]
		</p>
		</div>
	</li>
	<li>
		<div>
		<a href="https://arxiv.org/abs/2111.12624">Self-slimmed Vision Transformer</a>
		<br>Zhuofan Zong*, <b>Kunchang Li*</b>, Guanglu Song, Yali Wang, Yu Qiao, Biao Leng, Yu Liu.<br>
		<em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2022.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2111.12624">Paper</a>]
			[<a href="https://github.com/Sense-X/SiT">Code</a>]
		</p>
		</div>
	</li>
	<li>
		<div>
		<a href="https://arxiv.org/abs/2111.03930">Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling</a>
		<br>Renrui Zhang*, Rongyao Fang*, Wei Zhang*, Peng Gao, <b>Kunchang Li</b>, Jifeng Dai, Yu Qiao, Hongsheng Li.<br>
		<em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2022.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2111.03930">Paper</a>]
			[<a href="https://github.com/gaopengcuhk/Tip-Adapter">Code</a>]
		</p>
		</div>
	</li>
	<li>
		<div class="highlight">
		<a href="https://arxiv.org/abs/2201.04676">UniFormer: Unified Transformer for Efficient Spatiotemporal Representation Learning</a>
		<br><b>Kunchang Li*</b>, Yali Wang*, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, Yu Qiao.<br>
		<em>International Conference on Learning Representations</em> (<b>ICLR</b>), 2022.  &nbsp <font color="red"><strong>(Top 3% of review scores)</strong></font></br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2201.04676">Paper</a>]
			[<a href="https://github.com/Sense-X/UniFormer">Code</a>]
			[<a href="https://huggingface.co/spaces/Andy1621/uniformer_light">Demo</a>]
			[<a href="https://zhuanlan.zhihu.com/p/461361343">Blog</a>]
		</p>
		</div>
	</li>
	<li>
		<div>
		<a href="https://arxiv.org/abs/2106.01603">CT-Net: Channel Tensorization Network for Video Classification</a>
		<br><b>Kunchang Li*</b>, Xianhang Li*, Yali Wang*, Jun Wang, Yu Qiao.<br>
		<em>International Conference on Learning Representations</em> (<b>ICLR</b>), 2021.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2106.01603">Paper</a>]
			[<a href="https://github.com/Andy1621/CT-Net">Code</a>]
		</p>
		</div>
	</li>
</ul>
</div>

<div id="internships">
<h2>Internships</h2>
<ul>
	<li>
		<div style="float:left; text-align:left"><b>OpenGVLab, Shanghai AI Lab</b>, Shanghai, China</div> <div style="float:right; text-align:right">Nov. 2021 ‚Äì Present</div><br>
		Research Intern<br>
		Advisor: <a href="https://scholar.google.com/citations?user=hD948dkAAAAJ">Yali Wang</a>, <a href="https://scholar.google.com/citations?user=HEuN8PcAAAAJ">Limin Wang</a>, <a href="https://scholar.google.com/citations?user=Xm2M8UwAAAAJ">Yi Wang</a><br>
		Topic: General Video Foundation Model; Large-scale Pre-training; Multimodal Learning<br>
	</li>
	<li>
		<div style="float:left; text-align:left"><b>X-Lab, SenseTime</b>, Beijing, China</div> <div style="float:right; text-align:right">Feb. 2021 ‚Äì Nov. 2021</div><br>
		Research Intern<br>
		Advisor: <a href="https://scholar.google.com/citations?user=Bd3v08QAAAAJ">Guanglu Song</a>, <a href="https://scholar.google.com/citations?user=Lzb6A1sAAAAJ">Yu Liu</a><br>
		Topic: Efficient Architecture Design; Video Understanding<br>
	</li>
	<li>
		<div style="float:left; text-align:left"><b>Megvii</b>, Beijing, China</div> <div style="float:right; text-align:right">Oct. 2019 ‚Äì Jan. 2020</div><br>
		Research Intern<br>
		Topic: Model Reproduction for MegEngine<br>
	</li>
</ul>
</div>


<div id="honors">
<h2>Honors</h2>
<ul>
	<li>
		<div style="float:left; text-align:left"><b>Top 10 Outstanding</b> Graduate Students in SIAT, UCAS</div> <div style="float:right; text-align:right">2023</div>
	</li>
	<li>
		<div style="float:left; text-align:left"><b>National award scholarship</b></div> <div style="float:right; text-align:right">2023</div>
	</li>
	<li>
		<div style="float:left; text-align:left"><b>Dean Scholarship</b> of University of Chinese Academy of Sciences</div> <div style="float:right; text-align:right">2022, 2023</div>
	</li>
	<li>
		<div style="float:left; text-align:left"><b>Merit Student</b> at University of Chinese Academy of Sciences</div> <div style="float:right; text-align:right">2021, 2022</div>
	</li>
	<li>
		<div style="float:left; text-align:left"><b>1st</b> Place in Forecasting Challenge (ECCV2022 Ego4D Workshop)</div> <div style="float:right; text-align:right">2022</div>
	</li>
	<li>
		<div style="float:left; text-align:left"><b>2nd</b> Place in Action Recognition in the Dark Challenge (CVPR2022 UG2+ Workshop)</div> <div style="float:right; text-align:right">2022</div>
	</li>
	<li>
		<div style="float:left; text-align:left"><b>1st</b> Place in Semantic Segmentation of Remote Sensing Images (CCF BDCI Contest)</div> <div style="float:right; text-align:right">2021</div>
	</li>
	<li>
		<div style="float:left; text-align:left"><b>Comprehensive Grand Prize</b> of CCF BDCI Contest</div> <div style="float:right; text-align:right">2021</div>
	</li>
	<li>
		<div style="float:left; text-align:left"><b>Excellent</b> Higher Education Graduate of Beijing Municipality</div> <div style="float:right; text-align:right">2020</div>
	</li>
	<li>
		<div style="float:left; text-align:left">Grand Prize of Social Work Scholarship, Grand Prize of Study Excellence Scholarship, <br>Merit Student and
			Honor student at Beihang University</div> 
		<div style="float:right; text-align:right">2017, 2018, 2019</div>
		
	</li>
</ul>
</div>

<br>

<div id="services">
	<h2>Services</h2>
	<ul>
		<li>Conference Reviewer: ICLR2023/2024, CVPR2023/2024, ICCV2023, NeurIPS2023/2024, ICML2024, ECCV2024
		<li>Conference PC: AAAI2025
		<li>Journal Reviewer: TPAMI, IJCV, PR, NN, JVCI
		<li>Talk: AI Drive 2022, AI Time 2022, AI Time 2023, others (FDU, UCAS, KAUST...)
	</ul>
</div>


<div id="footer">
	<div id="footer-text"></div>
	<center><a href="https://www.easycounter.com/"><img src="https://www.easycounter.com/counter.php?andy1621" border="0" alt="HTML Counter"></a></center>
</div>
	<center>¬© Kunchang Li | Last updated: 08/01/2024</center>
</div>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-88615920-1', 'auto');
  ga('send', 'pageview');

</script>

</div>
</body>
</html>